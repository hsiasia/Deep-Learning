# -*- coding: utf-8 -*-
"""RNN(TextGeneration).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cJ8NNcKOCi32zYY7IFpKCEc7DLQwlL0o

# RNN ： AI二次創作
爬蟲來源: [crawl_book](https://colab.research.google.com/drive/1f_HvQEvgkJPFc473TlA-I_3EmkThA2SR?usp=sharing)

程式碼參考: [Tensorflow](https://www.tensorflow.org/tutorials/text/text_generation)

本次資料集，著作權乃是張愛玲女士所擁有。**請勿將本次資料集散播、更改、用於非商業用途**。

> **訓練步驟**

深度學習在訓練模型上有以下幾個重要的步驟:
1. 讀入相關封包
2. 取得資料集 
3. 資料前處理
4. 建立模型
5. 制定訓練計畫
6. 評估模型
7. 做預測
"""

# ****************************************
# **請勿將本次資料集散播、用於非商業用途**
# ****************************************

# 執行即代表同意將會合法、合理使用資料集

!wget -O Eileen_Legendary.txt "http://140.115.82.54/NN/Recurrent/Eileen_Legendary.txt"

# 助教提供的爬蟲程式,使用文本為東野圭吾的放學後
# 開始執行前請先在心中默念著作權相關法律，並發誓只將資料集用於教學用途
# 請下載失去著作權保護期的書籍 (50年)
# 執行即代表已完成閱讀著作權辦法，並同意遵循著作權相關條例

import requests
from lxml import etree
import re 
import urllib
from tqdm import tqdm
import os

if(os.path.isdir("./books") == False):
  os.mkdir("./books")

def get_book_information(url):
  url_chinese = urllib.parse.unquote(url)
  url_re = re.compile(r'com/(.*)\.php')
  url_re2 = re.compile(r'.*\.php')
  url_re3 = re.compile(r'\.(html|xhtml)')
  save_path = "./books/"+url_re.search(url_chinese).group(1).strip()+".txt"
  url_book_domain = url_re2.match(url).group(0)
  html_format = url_re3.search(url).group(0)
  print("儲存位置: "+save_path)
  return save_path,url_book_domain,html_format

def get_pages_of_book(url):
  response = requests.get(url+"#book_toc")
  html = etree.HTML(response.content)
  content_number = len(html.xpath('.//div[@data-role="content"]//ul/li'))
  return content_number

def download_one_book(url):
  save_path,url_book_domain,html_format = get_book_information(url)
  content_number = get_pages_of_book(url)
  page_start = 2 if(html_format == ".html") else 1
  page_end = page_start+content_number
  file = open(save_path,"w",encoding="utf8")
  for page in tqdm(range(page_start,page_end)):
      if(html_format == ".html"):
          url_ = url_book_domain+"/"+(str(page) if page>9 else "0"+str(page))+html_format
      else:
          url_ = url_book_domain+"/"+str(page)+html_format
      response = requests.get(url_)
      html = etree.HTML(response.content.decode("utf-8","replace"))
      if(html_format == ".html"):
          content = html.xpath('.//div[@data-role="content"]/p/text()')
      else:
          content = html.xpath('.//div[@data-role="content"]/div/text()')
      assey = [a.strip().replace("\u3000"," ") for a in content]
      file.write("\n".join(assey)+"\n")
  file.flush()
  file.close()

url = "https://www.bookscool.com/%E3%80%8A%E6%94%BE%E5%AD%B8%E5%BE%8C%E3%80%8B.php/1.xhtml"
download_one_book(url)

def download_many_books(url):
  response = requests.get(url)
  html = etree.HTML(response.content)
  links = html.xpath('.//div[@data-role="content"]/a/@href')
  links = list(filter(lambda link:re.match("http",link) == None,links))
  for l in links:
      link = "https://www.bookscool.com/"+l
      download_one_book(link)

"""## 1. 讀入Package"""

import numpy as np
import tensorflow as tf
import pandas as pd
from tensorflow import keras
from tensorflow.keras import layers
import os
import matplotlib.pyplot as plt

"""## 2. 取得資料集"""

book = ""
with open("./books/《放學後》.txt","r",encoding="utf8") as file:
  for line in file:
    book += line

book_length = len(book)
unique_words = set(book)
print(f"放學後共有 {book_length} 字詞")
print(f"包含了 {len(unique_words)} 個獨一無二的字 (含標點符號)\n")
print(book[0:500])

"""## 3. 資料前處理

文字前處理有一堆方法、作法:
* 切字
* 還原
* 清除特殊字符
* 清除不常見字符 (StopWord)


我這裡僅使用去除不常見的字(StopWord)
"""

# 計算字數統計
words_count = {}
for w in book:
  if w in words_count:
    words_count[w] += 1
  else:
    words_count[w] = 1

words_count = sorted(words_count.items(),key=lambda x:x[1])

stop_word = 8
unique_words = [w_tup[0] for w_tup in words_count if w_tup[1]>stop_word]
print(f"去除次數小於{stop_word}的文字剩餘 : {len(unique_words)}")

print(f"原本放學後共有 {book_length} 字詞")
print(f"去除不常出現的文字後")
book = [w for w in book if w in unique_words]
print(f"剩餘{len(book)}個字")

# 文字轉數字(index)
word_2_index = {word:index for index,word in enumerate(unique_words)}
index_2_word = {word_2_index[word]:word for word in word_2_index}

book_2_index = [word_2_index[w] for w in book]

print("原始文字 : ")
print(book[:40])
print("-"*40)
print("轉成index : ")
print({word_2_index[w] for w in book[:40]})

def ind2word_seq(seq):
  return [index_2_word[i] for i in seq]

# 設定輸入模型長度
seq_len = 20
characters = tf.data.Dataset.from_tensor_slices(book_2_index)

sequences = characters.batch(seq_len+1,drop_remainder=True)

for seq in sequences.take(2):
  print(seq.shape)
  print(seq)
  print([index_2_word[i] for i in seq.numpy()])

# 做input、target切割
def split_input_target(seq):
  input_txt = seq[:-1]
  target_txt = seq[1:]
  return input_txt,target_txt

split_input_target(list("Tensorflow"))

dataset = sequences.map(split_input_target)

for input_example,target_exaple in dataset.take(2):
  print("Input :", ind2word_seq(input_example.numpy()))
  print("Target:", ind2word_seq(target_exaple.numpy()))
  print("-"*50)
  print("Input :", input_example.numpy())
  print("Target:", target_exaple.numpy())

# 建立資料集
# Batch size
BATCH_SIZE = 64

BUFFER_SIZE = 10000

dataset = (
    dataset
    .shuffle(BUFFER_SIZE)
    .batch(BATCH_SIZE, drop_remainder=True))

dataset

# 超參數
EMBEDDING_DIM = 512

# 使用 keras 建立一個非常簡單的 LSTM 模型
model = tf.keras.Sequential()

model.add(
  tf.keras.layers.Embedding(
    input_dim=len(unique_words), 
    output_dim=EMBEDDING_DIM
))

model.add(
  tf.keras.layers.LSTM(
    units=4096, 
    return_sequences=True, 
))

model.add(
  tf.keras.layers.LSTM(
    units=2048, 
    return_sequences=True,
))
  
model.add(
  tf.keras.layers.Dense(
      len(unique_words),activation="softmax"))

model.summary()

# 查看模型的輸入、輸出 shape
for input_example,target_exaple in dataset.take(1):
  predict_example = model(input_example)
  print(f"Model input shape : {input_example.shape}")
  print(f"Model output shape : {predict_example.shape}")
  print(f"Model target shape : {target_exaple.shape}")

print("原本的中文字序列：")
[print(index_2_word[ind],end="") for ind in input_example[0].numpy()]
print()
print("-"*40)
print("輸入尚未訓練的model後獲得：")
print()

predict_words = tf.math.argmax(predict_example[0],-1)
[print(index_2_word[ind],end="") for ind in predict_words.numpy()]
print()

"""## 5. 制定訓練計畫並訓練

* [sparse_categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy) V.S. [categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy)

```python=
# categorical_crossentropy
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
assert loss.shape == (2,)

# sparse_categorical_crossentropy
y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
assert loss.shape == (2,)

```

"""

model.compile(loss="sparse_categorical_crossentropy",optimizer="adam")

EPOCHS = 20
history = model.fit(
    dataset, # 前面使用 tf.data 建構的資料集
    epochs=EPOCHS,
)

"""## 6. 衡量模型"""

plt.plot(history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')

plt.show()

after_train_predictions = model(input_example)
after_sampled_indices = tf.argmax(after_train_predictions[0],1)

print("原本的中文字序列：")
[print(index_2_word[ind],end="") for ind in input_example[0].numpy()]
print()
print("-"*40)
print("輸入進訓練後的model後獲得：")
print()

[print(index_2_word[ind],end="") for ind in after_sampled_indices.numpy()]
print()

"""## 7. 做預測

![](https://i.imgur.com/YsOj6Mw.png)

在實際生成文字時，我們會想要增加一些隨機性。比如”天天出去” 不加入隨機 “天天天天” 如果我們全部輸出的字都是取softmax最大可能性，則一個訓練完美的model會把整本書給輸出出來。但是我們要的是，希望電腦在最大可能性的幾個字中隨機挑選一個字出來。

tf.random.categorical 會根據softmax機率後隨機挑選字，但是我們不希望因為模型很爛導致不合理的字被選中，因此我們會除上一個temperature來增加可能字的比重。

EX: "天天出去" 預測下一個字
1. 玩 : 0.3 
2. 天 : 0.1 
3. 浪 : 0.4 

"天"有的機率被印出，我們不希望。所以我們可以在每一個機率除上一個temperature(0.01)
1. 玩 : 30 
2. 天 : 10 
3. 浪 : 40 
原本"浪"跟"天"差0.3，除temperature後差30


"""

# 預測文字，並把預測文字循環當作下一次的輸入
# 設定你的temperature
temperature = 0.01

def generateWords(input,words=500):
  [print(index_2_word[ind],end="") for ind in input]
  for i in range(words):
    next_input = tf.expand_dims(input,axis=0)
    predicts = model(next_input)
    predicts = predicts[:,-1,:]
    predicts /= temperature
    result = tf.random.categorical(
        predicts,num_samples=1
    )
    chinese_ind = tf.squeeze(result).numpy()
    print(index_2_word[chinese_ind],end="")
    input = input+[chinese_ind]
    input = input[-seq_len:]

init_seq = "教室"
init_seq_ind = [word_2_index[w] for w in init_seq]
input = init_seq_ind[-seq_len:]

generateWords(input,500)